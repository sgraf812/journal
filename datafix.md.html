<meta charset="utf-8" emacsmode="-*- markdown -*-">

# Thoughts about the `datafix` approach

## Why do we need the outer `ProblemSpec` monad? Can't we just dynamically detect dependencies?

- We need to allocate nodes in the data-flow graph  
- We can do that in a way that doesn't leak into the UI, but we still have to construct the problem in some kind of 'FrameworkDescription' (to put it in terms of `reactive-banana`)
- All attempts failed to support a signature à la `datafix :: (LiftFunc (Domain m) m -> LiftFunc (Domain m) m) -> LiftFunc (Domain m) m`  
  - (That doesn't consider change detection, but we can always thread that through)
  - We need to allocate nodes *before* actually executing the actions, otherwise we can't tie the knot 
  - Only way out: An ugly hack based on `unsafePerformIO (newIORef UninitiailizedNode)`
  - Doesn't work as expected when we call `datafix` inside of `datafix`: It will allocate a new node on each dynamic call, rather than cache among 'syntactically identical' occurrences
  - I.o.w.: It's not `Reader`-like state, but rather actual `State` (cache the node id on subsequent iterations) that we can't handle referentially transparent without leaking into the UI
- At least we don't need to leak node ids to the user interface (I think), so the monad should be pretty opaque, like it is for RB

## How does this compare to `reactive-banana`?

- Setup happens in `MomentIO = ReaderT EventNetwork Build`, [`Build`](https://hackage.haskell.org/package/reactive-banana-1.1.0.1/docs/src/Reactive-Banana-Prim-Types.html#Build)
- Graph based on `HashMap a [a]`, nodes for `Pulse a`s, e.g. each `Event`. So, something constructed by `apply` will be a distinct node in the graph
- These kinds of edges are tracked and topologically sorted when the network is finally build

## As `MonadFix`

- `datafix` is really a restricted form of `MonadFix`, where `a ~ LiftInto a m`
- Would love it if `mdo` was part of `-XRebindableSyntax`, but it isn't
- So we can't use `mdo` as it stands, but we can mimic the experience of `mfix`

## Separate Builder monad from the monad we do computations in

|Pros | Cons|
|-----|-----|
| SOC | less flexible: We might want to `mfix` new functions depending on data-flow (e.g. applicative vs. monadic) |
| unforeseeable repercussions | Don't know how to do this in a way that doesn't expose implementation details |
| There's the hope of the compiler doing a better job at floating bindings out | What about the specification? DFP looks like `m (func m)`. We can't get at the func this way, it might be guarded by a dependOn |

## The signature of datafix

- Outer `Builder` is necessary by the last section
- How do we enable mutually recursive groups?  
  - Just `(Func -> Builder Func) -> Builder Func` doesn't work: we can't get out any result out of the functional unless we fully apply it
  - Full application is a no-go:  
    1) we compute stuff repeatedly we don't need to 
    2) it mixes `Builder` phase with `DependencyM` phase
  - Ideally, we want `datafix` to also return a value in `Builder`
  - Rather than fixing what we return to `Func`, we should instead return an arbitrary `a` the user supplies
  - So, instead of `mfix :: (a -> m a) -> m a` we have `datafixEq :: (Func -> Builder (a, Func)) -> Builder a`

## Why not `dependOn`?

- Seems like the cleaner/simpler primitive, but leaks `Node`s into UI
- Also enables strictly more scenarios, by leaving allocation to the user
- Outer abstractions are free to build whatever they want
- Maybe pass around some partially applied version, like `LetDn` provides?
- Or just formulate the `datafix` primitive in terms of the `dependOn` abstraction? Is that possible?  
  - Means we also have to provide the surrounding Builder infrastructure, which we already have anyway
  - Compile `(MonadDatafix mdat, MonadDependency mdep) => Builder (LiftFunc domain mdat) -> DFP Dense domain` which can be passed on to `solveProblem`

## What algorithm to employ in `recompute`

- *At the least* we want to descend into $\bot$ nodes, so that we discover the set of reachable nodes quickly
- Potentially we could always descend into nodes until we hit a cycle  
  - so do depth-first search for all nodes, not just $\bot$ ones
- So, effectively there are 3 degrees wrt. how eager we are to descend into nodes:  
  1. Don't descend at all, let the worklist handle this  
    - This means that we need O(n) iterations of the worklist just to discover stuff
    - We assume that enqueing and dequeing nodes ($\mathcal{O}(\log(n))$) in the worklist is less efficient than just descending into the node directly!
    - Also when one dependency is $\bot$, there may be nodes which aren't discovered because of that
    - Consider a non-recursive expression, where we could get away with a single depth-first traversal: The worklist-only approach would need to `recompute` a let node at least twice
    - A trivial optimisation would be to descend into nodes we haven't discovered before, e.g. which are $\bot$ and not present in the call stack (cycles)
  2. Descend into $\bot$ (or rather undiscovered) nodes if they aren't already in the call stack  
    - Discovers set of reachable nodes quickly, so that the worklist algorithm sees all reachable nodes and can figure out a good iteration order
  3. Descend into every non-stable node, modulo breaking cycles at some point (typically after the first iteration)  
    - Worklist algorithm would probably the better authority to decide which non-stable node to `recompute` next, given it knows all relevant nodes
    - At some point this corresponds to what the ad-hoc analyses in GHC do: Iterate a single cycle until all nodes are stable
    - Only difference: We don't really know up-front how many iterations to do.. Would entail iterating in `dependOn`??? Probably not viable
- So, all in all, the second scheme is probably the way to go about this
- Plan:  
  1. Implement something that can handle all three schemes
  2. Benchmark variants
  3. My gut says 2. will win. If so, tune implementation for that.

### How an algorithm that can do 3. looks like

- Realised in [`d473a14`](https://github.com/sgraf812/datafix/tree/d473a140966b499d49d1b126cfd0cdb37c34b226)
- We need fine-grained tracking of *when* a node update influences a dependency
- Consider a node `A` that `dependOn` `B` twice
- The first change of `B` doesn't make unstable, but if we employ scheme 3, the second must do so!
- So:  
  - At the begin of `recompute`, we have to delete the node from the worklist
  - All dependency edges are cleared! Only re-added *after* the first `dependOn`
  - This makes sure that the first `dependOn` in the example doesn't mark `A` as unstable, but the second does
- Outline for `recompute A`:  
  1. delete `A` from worklist
  2. cons `A` to the call stack
  3. 'focus' on the `NodeInfo` of `A`
  4. delete all references and the corresponding referrer records in references ($\mathcal{O}(d*\log(n))$ where $d$ is the max degree of a node and $n$ is the number of nodes)
  5. invoke transfer function
  6. Update node value
  7. Compare with old value by change detector
  8. Enqueue referrers
  9. Pop the activation record, return the value from 5.
- Outline for `dependOn B` (encodes the actual scheme (1-3) above):
  1. When there's a cycle, return $\bot$
  2. When there's a stable value, return that value
  3. `B` is either unstable or not even discovered, but not present in the call stack. Now depending on the scheme (1-3): 
    1. Return $\bot$ (or a more precise optimistic approximation), note newly discovered nodes as unstable.
    2. If not discovered, `recompute`. Otherwise return optimistic approximation.
    3. Always `recompute`.
  4. Only now add `B` to the references of caller `A` and vice versa with referrer

### How an algorithm that only can do 1. or 2. looks like

- Observation: While a node is `recompute`d, its value is never invalidated, *except* through self-invalidation through a loop  
  - This is because only the first `dependOn` of a node might lead to a `recompute`, so the [double dependency case](https://github.com/sgraf812/datafix/blob/d473a140966b499d49d1b126cfd0cdb37c34b226/test/Critical.hs#L59-L77) doesn't happen
  - This allows us to mark the node as stable *after* a `recompute` and be more lax about when to update referrers and references
  - Basically the old scheme used during [my thesis](https://github.com/sgraf812/ghc/blob/6f9f06c2d1bf3a9168ec4079ebf6da26398e54b9/compiler/utils/Worklist.hs#L88)  
    - Also explains why I failed to implement scheme 3 (the stuff I commented out): That's simply not possible in this formulation
  - Entails computing a `Diff` of changed `references` at the end of each `recompute`
  - Basically postpone accesses of referrers so that we can handle them in one batch
- So, the outline for `dependOn B` changes in step 4:  
  4. Add `B` to a set of `references` of the currently called node `A`
- Outline of `recompute A`:
  1. 'Push' a new 'activation record' (cons `A` to the call stack, clear `references`)
  2. Invoke transfer function
  3. Update node value
  4. Compare with old value by change detector
  5. Enqueue referrers from some state *after* invoking the transfer function  
  6. Enqueue `A` itself if it is part of the new `references`  
      - This awkward special case accounts for self-loops!
  7. Pop the activation record, return the value from 2.

## Comparison/Concepts from Datalog

- Uses declarative logic language like Prolog to specify analyses
- Faster (!) than C versions!
- Related: Dyna (implemented in Haskell), bddbddb, LogicBlox

### Practical program analysis using general purpose logic programming systems—a case study

- Applies this approach to strictness analysis
- Section 2: 'Analysis methods are usually described in terms of semantic equations whose fix point represents the program property under consideration. These semantic equations can be viewed as a (constraint) logic program, where each equation is translated into a Horn clause.'
- Section 2 is VERY interesting! Suggests that `TrasferFunction` is nothing else than a formulation of this logic system
- Section 3.2: Demand propagation (like transformer)
- predicate $sp_f(D, X)$ for function $f(x)$: incoming demand $D$ propagates to argument demand $X$ on $x$
- Performance in 4.3: 5% of time taken by ghc, although this was in 1996...

### From Datalog to flix: a declarative language for fixed points on lattices (2016)

- VERY VERY related
- Datalog dialect with proper lattices and ops
- DSL
- Rather slow, due to interpreter. FW: compile to JVM bytecode, query planning, index selection, etc.
- JVM, so not really usable
- Approach remarkable, mine droven to extreme

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>